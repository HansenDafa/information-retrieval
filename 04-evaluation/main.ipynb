{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exercise 8.8 ---\n",
      "a. MAP System 1: 0.60\n",
      "   MAP System 2: 0.49\n",
      "   System with higher MAP: System 1\n",
      "\n",
      "b. Intuitive Sense: Yes, MAP rewards early retrieval of relevant docs.\n",
      "   System 1 found relevant docs at ranks 1 and 3, boosting its score early.\n",
      "   System 2 found relevant docs starting later (rank 2, 5, 6, 7).\n",
      "\n",
      "c. R-Precision System 1 (R=4): 0.50\n",
      "   R-Precision System 2 (R=4): 0.25\n",
      "   Ranking based on R-Precision (System 1 > System 2) is the same as MAP.\n",
      "\n",
      "--- Exercise 8.9 ---\n",
      "a. Precision@20: 0.30\n",
      "b. F1@20: 0.4286\n",
      "c. Uninterpolated Precision at 25.0% Recall: 1.00\n",
      "d. Interpolated Precision at 33% Recall: 0.36\n",
      "e. MAP (Standard AP, denominator=8): 0.4163\n",
      "   MAP (Report's method for point e, denominator=6): 0.5551\n",
      "f. Largest possible MAP: 1.0\n",
      "g. Smallest possible MAP (from report): 0.00045\n",
      "h. Error vs Max MAP: |0.5551 - 1.0| = 0.4449\n",
      "   Error vs Min MAP: |0.5551 - 0.00045| = 0.5546\n",
      "   Largest absolute error: 0.5546 (or 55.5%)\n",
      "\n",
      "--- Exercise 8.10 ---\n",
      "a. Kappa measure: -0.333\n",
      "\n",
      "b. Relevant if both judges agree:\n",
      "   Relevant Set: {3, 4}\n",
      "   Retrieved & Relevant: {4}\n",
      "   Precision: 1/5 = 0.200\n",
      "   Recall:    1/2 = 0.500\n",
      "   F1:        0.286\n",
      "\n",
      "c. Relevant if either judge agrees:\n",
      "   Relevant Set (Size): 10\n",
      "   Retrieved & Relevant: {4, 5, 6, 7, 8}\n",
      "   Precision: 5/5 = 1.000\n",
      "   Recall:    5/10 = 0.500\n",
      "   F1:        0.667\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Helper function to convert R/N lists to 1/0 lists\n",
    "def rn_to_binary(results_list):\n",
    "  \"\"\"Converts a list of 'R'/'N' strings to a list of 1/0 integers.\"\"\"\n",
    "  return [1 if item == 'R' else 0 for item in results_list]\n",
    "\n",
    "# --- Metric Calculation Functions ---\n",
    "\n",
    "def calculate_ap(results_binary, total_relevant):\n",
    "  \"\"\"Calculates Average Precision (AP) for a single query.\"\"\"\n",
    "  if total_relevant == 0:\n",
    "    return 0.0\n",
    "\n",
    "  precision_sum = 0.0\n",
    "  relevant_count = 0\n",
    "  for k, is_relevant in enumerate(results_binary):\n",
    "    if is_relevant:\n",
    "      relevant_count += 1\n",
    "      precision_at_k = relevant_count / (k + 1)\n",
    "      precision_sum += precision_at_k\n",
    "\n",
    "  # Assume 0 precision for any relevant docs not retrieved (though not needed for these examples explicitly)\n",
    "  return precision_sum / total_relevant\n",
    "\n",
    "def calculate_r_precision(results_binary, total_relevant):\n",
    "  \"\"\"Calculates R-Precision.\"\"\"\n",
    "  if total_relevant == 0:\n",
    "      return 0.0\n",
    "  if len(results_binary) < total_relevant:\n",
    "      # Handle cases where fewer docs are returned than R (not in these examples)\n",
    "      # This might require padding or specific handling based on definition used\n",
    "      print(f\"Warning: Fewer results ({len(results_binary)}) than R ({total_relevant}). R-Prec might be ill-defined.\")\n",
    "      # Assuming we calculate precision based on available results up to R or len(results), let's use min\n",
    "      limit = min(len(results_binary), total_relevant)\n",
    "      if limit == 0: return 0.0\n",
    "      relevant_in_top_r = sum(results_binary[:limit])\n",
    "      return relevant_in_top_r / limit # Adjust definition if needed\n",
    "  else:\n",
    "      relevant_in_top_r = sum(results_binary[:total_relevant])\n",
    "      return relevant_in_top_r / total_relevant\n",
    "\n",
    "\n",
    "def calculate_precision_recall_f1(retrieved_count, relevant_found_count, total_relevant_in_collection):\n",
    "    \"\"\"Calculates Precision, Recall, and F1 given counts.\"\"\"\n",
    "    if retrieved_count == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = relevant_found_count / retrieved_count\n",
    "\n",
    "    if total_relevant_in_collection == 0:\n",
    "        recall = 0.0 # Or undefined, depending on context\n",
    "    else:\n",
    "        recall = relevant_found_count / total_relevant_in_collection\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calculate_uninterpolated_precision_at_recall(results_binary, total_relevant, target_recall_level):\n",
    "    \"\"\"Calculates uninterpolated precision at a specific recall level.\"\"\"\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    target_relevant_count = math.ceil(target_recall_level * total_relevant)\n",
    "    if target_relevant_count == 0: # Handle 0% recall if needed\n",
    "        return 1.0 # Often defined as 1 at 0 recall, or handle as edge case\n",
    "\n",
    "    relevant_count = 0\n",
    "    for k, is_relevant in enumerate(results_binary):\n",
    "        if is_relevant:\n",
    "            relevant_count += 1\n",
    "            if relevant_count >= target_relevant_count:\n",
    "                 # Found the first point at or past the target recall\n",
    "                 precision_at_k = relevant_count / (k + 1)\n",
    "                 return precision_at_k\n",
    "    # If target recall is never reached\n",
    "    return 0.0\n",
    "\n",
    "def calculate_interpolated_precision_at_recall(results_binary, total_relevant, target_recall_level):\n",
    "    \"\"\"Calculates interpolated precision at a specific recall level.\"\"\"\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    relevant_count = 0\n",
    "    for k, is_relevant in enumerate(results_binary):\n",
    "        if is_relevant:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / (k + 1)\n",
    "            recall_at_k = relevant_count / total_relevant\n",
    "            precisions.append(precision_at_k)\n",
    "            recalls.append(recall_at_k)\n",
    "\n",
    "    max_precision = 0.0\n",
    "    for p, r in zip(precisions, recalls):\n",
    "        if r >= target_recall_level:\n",
    "            max_precision = max(max_precision, p)\n",
    "\n",
    "    return max_precision\n",
    "\n",
    "def calculate_kappa(judgments):\n",
    "    \"\"\"Calculates Cohen's Kappa for inter-judge agreement.\n",
    "    Args:\n",
    "        judgments: A list of tuples, where each tuple is (judge1_score, judge2_score).\n",
    "                   Scores are assumed to be 0 (non-relevant) or 1 (relevant).\n",
    "    Returns:\n",
    "        The kappa statistic.\n",
    "    \"\"\"\n",
    "    n = len(judgments)\n",
    "    if n == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Contingency Table\n",
    "    agree_r = 0 # Both 1\n",
    "    agree_n = 0 # Both 0\n",
    "    j1r_j2n = 0 # Judge 1 = 1, Judge 2 = 0\n",
    "    j1n_j2r = 0 # Judge 1 = 0, Judge 2 = 1\n",
    "\n",
    "    # Marginal counts\n",
    "    j1_r_count = 0\n",
    "    j1_n_count = 0\n",
    "    j2_r_count = 0\n",
    "    j2_n_count = 0\n",
    "\n",
    "    for j1, j2 in judgments:\n",
    "        if j1 == 1 and j2 == 1:\n",
    "            agree_r += 1\n",
    "        elif j1 == 0 and j2 == 0:\n",
    "            agree_n += 1\n",
    "        elif j1 == 1 and j2 == 0:\n",
    "            j1r_j2n += 1\n",
    "        elif j1 == 0 and j2 == 1:\n",
    "            j1n_j2r += 1\n",
    "\n",
    "        if j1 == 1: j1_r_count += 1\n",
    "        else: j1_n_count += 1\n",
    "        if j2 == 1: j2_r_count += 1\n",
    "        else: j2_n_count += 1\n",
    "\n",
    "    # Observed Agreement (Po)\n",
    "    po = (agree_r + agree_n) / n\n",
    "\n",
    "    # Expected Agreement (Pe)\n",
    "    p_j1_r = j1_r_count / n\n",
    "    p_j1_n = j1_n_count / n\n",
    "    p_j2_r = j2_r_count / n\n",
    "    p_j2_n = j2_n_count / n\n",
    "    pe = (p_j1_r * p_j2_r) + (p_j1_n * p_j2_n)\n",
    "\n",
    "    # Kappa\n",
    "    if 1 - pe == 0: # Avoid division by zero if perfect agreement is expected by chance\n",
    "        return 1.0 if po == 1.0 else 0.0 # Or handle as undefined/special case\n",
    "\n",
    "    kappa = (po - pe) / (1 - pe)\n",
    "    return kappa\n",
    "\n",
    "# --- Exercise 8.8 ---\n",
    "print(\"--- Exercise 8.8 ---\")\n",
    "results_s1_rn = ['R', 'N', 'R', 'N', 'N', 'N', 'N', 'N', 'R', 'R']\n",
    "results_s2_rn = ['N', 'R', 'N', 'N', 'R', 'R', 'R', 'N', 'N', 'N']\n",
    "total_relevant_8_8 = 4\n",
    "\n",
    "results_s1_bin = rn_to_binary(results_s1_rn)\n",
    "results_s2_bin = rn_to_binary(results_s2_rn)\n",
    "\n",
    "# a. MAP\n",
    "ap_s1 = calculate_ap(results_s1_bin, total_relevant_8_8)\n",
    "ap_s2 = calculate_ap(results_s2_bin, total_relevant_8_8)\n",
    "# Since there's only one \"query\" per system result, AP = MAP for that system\n",
    "print(f\"a. MAP System 1: {ap_s1:.2f}\")\n",
    "print(f\"   MAP System 2: {ap_s2:.2f}\")\n",
    "print(f\"   System with higher MAP: {'System 1' if ap_s1 > ap_s2 else 'System 2'}\")\n",
    "\n",
    "# b. Intuition (Explanation is qualitative)\n",
    "print(\"\\nb. Intuitive Sense: Yes, MAP rewards early retrieval of relevant docs.\")\n",
    "print(\"   System 1 found relevant docs at ranks 1 and 3, boosting its score early.\")\n",
    "print(\"   System 2 found relevant docs starting later (rank 2, 5, 6, 7).\")\n",
    "\n",
    "# c. R-Precision\n",
    "r_prec_s1 = calculate_r_precision(results_s1_bin, total_relevant_8_8)\n",
    "r_prec_s2 = calculate_r_precision(results_s2_bin, total_relevant_8_8)\n",
    "print(f\"\\nc. R-Precision System 1 (R=4): {r_prec_s1:.2f}\")\n",
    "print(f\"   R-Precision System 2 (R=4): {r_prec_s2:.2f}\")\n",
    "print(f\"   Ranking based on R-Precision ({'System 1 > System 2' if r_prec_s1 > r_prec_s2 else 'System 2 > System 1'}) is the same as MAP.\")\n",
    "\n",
    "# --- Exercise 8.9 ---\n",
    "print(\"\\n--- Exercise 8.9 ---\")\n",
    "results_8_9_rn = ['R', 'R', 'N', 'N', 'N', 'N', 'N', 'N', 'R', 'N',\n",
    "                  'R', 'N', 'N', 'N', 'R', 'N', 'N', 'N', 'N', 'R']\n",
    "total_relevant_8_9 = 8\n",
    "results_8_9_bin = rn_to_binary(results_8_9_rn)\n",
    "retrieved_count_8_9 = 20\n",
    "relevant_found_8_9 = sum(results_8_9_bin)\n",
    "\n",
    "# a. Precision at top 20\n",
    "precision_at_20, _, _ = calculate_precision_recall_f1(retrieved_count_8_9, relevant_found_8_9, total_relevant_8_9)\n",
    "print(f\"a. Precision@20: {precision_at_20:.2f}\")\n",
    "\n",
    "# b. F1 at top 20\n",
    "_, _, f1_at_20 = calculate_precision_recall_f1(retrieved_count_8_9, relevant_found_8_9, total_relevant_8_9)\n",
    "print(f\"b. F1@20: {f1_at_20:.4f}\")\n",
    "\n",
    "# c. Uninterpolated Precision at 25% Recall\n",
    "recall_target_c = 0.25\n",
    "uninterp_prec_at_25_recall = calculate_uninterpolated_precision_at_recall(results_8_9_bin, total_relevant_8_9, recall_target_c)\n",
    "print(f\"c. Uninterpolated Precision at {recall_target_c*100}% Recall: {uninterp_prec_at_25_recall:.2f}\")\n",
    "\n",
    "# d. Interpolated Precision at 33% Recall\n",
    "recall_target_d = 0.33\n",
    "interp_prec_at_33_recall = calculate_interpolated_precision_at_recall(results_8_9_bin, total_relevant_8_9, recall_target_d)\n",
    "print(f\"d. Interpolated Precision at {recall_target_d*100:.0f}% Recall: {interp_prec_at_33_recall:.2f}\") # Match report format\n",
    "\n",
    "# e. MAP for the query (assuming top 20 is complete set)\n",
    "# Note: AP calculation uses total_relevant_in_collection, but for this specific question,\n",
    "# the report calculates AP based only on the 6 found relevant docs. Let's recalculate AP\n",
    "# considering only the 6 found ones as the denominator as per the report's method for this point.\n",
    "# However, the standard AP definition uses total_relevant_in_collection (8).\n",
    "# Let's use the standard definition first, then match the report.\n",
    "\n",
    "# Standard AP definition (denominator = 8)\n",
    "ap_8_9_std = calculate_ap(results_8_9_bin, total_relevant_8_9)\n",
    "print(f\"e. MAP (Standard AP, denominator=8): {ap_8_9_std:.4f}\")\n",
    "\n",
    "# AP as calculated in the report (denominator = 6, only considering found relevant docs)\n",
    "# Re-implementing the specific summation from the report for clarity\n",
    "precisions_at_relevant_ranks_8_9e = [1/1, 2/2, 3/9, 4/11, 5/15, 6/20]\n",
    "ap_8_9_report = sum(precisions_at_relevant_ranks_8_9e) / relevant_found_8_9 # Divide by 6 found\n",
    "print(f\"   MAP (Report's method for point e, denominator=6): {ap_8_9_report:.4f}\")\n",
    "\n",
    "# f. Largest possible MAP\n",
    "# Achieved when all 8 relevant docs are at ranks 1-8\n",
    "map_max_8_9 = 1.0\n",
    "print(f\"f. Largest possible MAP: {map_max_8_9:.1f}\")\n",
    "\n",
    "# g. Smallest possible MAP\n",
    "# Achieved when all 8 relevant docs are at ranks 9993-10000 in a 10k collection\n",
    "# Calculation from report:\n",
    "precisions_min = [i / (9992 + i) for i in range(1, 9)] # Approximation\n",
    "map_min_8_9_approx = sum(precisions_min) / 8\n",
    "map_min_8_9_report = 0.00045 # From report\n",
    "print(f\"g. Smallest possible MAP (from report): {map_min_8_9_report:.5f}\")\n",
    "# print(f\"   (Approximation: {map_min_8_9_approx:.5f})\") # Show calculation if needed\n",
    "\n",
    "# h. Largest error\n",
    "map_e = ap_8_9_report # Using the report's value for consistency\n",
    "error_vs_max = abs(map_e - map_max_8_9)\n",
    "error_vs_min = abs(map_e - map_min_8_9_report)\n",
    "max_error = max(error_vs_max, error_vs_min)\n",
    "print(f\"h. Error vs Max MAP: |{map_e:.4f} - {map_max_8_9:.1f}| = {error_vs_max:.4f}\")\n",
    "print(f\"   Error vs Min MAP: |{map_e:.4f} - {map_min_8_9_report:.5f}| = {error_vs_min:.4f}\")\n",
    "print(f\"   Largest absolute error: {max_error:.4f} (or {max_error*100:.1f}%)\")\n",
    "\n",
    "# --- Exercise 8.10 ---\n",
    "print(\"\\n--- Exercise 8.10 ---\")\n",
    "# Judgments: (docID, judge1, judge2) - Storing as list of tuples for kappa function\n",
    "judgments_list = [\n",
    "    (1, 0, 0), (2, 0, 0), (3, 1, 1), (4, 1, 1), (5, 1, 0), (6, 1, 0),\n",
    "    (7, 1, 0), (8, 1, 0), (9, 0, 1), (10, 0, 1), (11, 0, 1), (12, 0, 1)\n",
    "]\n",
    "# Store judgments in a dict for easier lookup by docID if needed later\n",
    "judgments_dict = {doc_id: (j1, j2) for doc_id, j1, j2 in judgments_list}\n",
    "\n",
    "retrieved_docs_8_10 = {4, 5, 6, 7, 8}\n",
    "retrieved_count_8_10 = len(retrieved_docs_8_10)\n",
    "\n",
    "# a. Kappa Measure\n",
    "kappa_score = calculate_kappa([(j1, j2) for _, j1, j2 in judgments_list])\n",
    "print(f\"a. Kappa measure: {kappa_score:.3f}\")\n",
    "\n",
    "# b. P, R, F1 (Relevant = Both judges agree)\n",
    "relevant_set_b = {doc_id for doc_id, j1, j2 in judgments_list if j1 == 1 and j2 == 1} # {3, 4}\n",
    "total_relevant_b = len(relevant_set_b)\n",
    "relevant_found_b = len(retrieved_docs_8_10.intersection(relevant_set_b)) # {4} -> count = 1\n",
    "\n",
    "precision_b, recall_b, f1_b = calculate_precision_recall_f1(retrieved_count_8_10, relevant_found_b, total_relevant_b)\n",
    "print(f\"\\nb. Relevant if both judges agree:\")\n",
    "print(f\"   Relevant Set: {relevant_set_b}\")\n",
    "print(f\"   Retrieved & Relevant: {retrieved_docs_8_10.intersection(relevant_set_b)}\")\n",
    "print(f\"   Precision: {relevant_found_b}/{retrieved_count_8_10} = {precision_b:.3f}\")\n",
    "print(f\"   Recall:    {relevant_found_b}/{total_relevant_b} = {recall_b:.3f}\")\n",
    "print(f\"   F1:        {f1_b:.3f}\")\n",
    "\n",
    "# c. P, R, F1 (Relevant = Either judge agrees)\n",
    "relevant_set_c = {doc_id for doc_id, j1, j2 in judgments_list if j1 == 1 or j2 == 1} # {3, 4, 5, 6, 7, 8, 9, 10, 11, 12}\n",
    "total_relevant_c = len(relevant_set_c)\n",
    "relevant_found_c = len(retrieved_docs_8_10.intersection(relevant_set_c)) # {4, 5, 6, 7, 8} -> count = 5\n",
    "\n",
    "precision_c, recall_c, f1_c = calculate_precision_recall_f1(retrieved_count_8_10, relevant_found_c, total_relevant_c)\n",
    "print(f\"\\nc. Relevant if either judge agrees:\")\n",
    "print(f\"   Relevant Set (Size): {total_relevant_c}\") # Print size for brevity\n",
    "print(f\"   Retrieved & Relevant: {retrieved_docs_8_10.intersection(relevant_set_c)}\")\n",
    "print(f\"   Precision: {relevant_found_c}/{retrieved_count_8_10} = {precision_c:.3f}\")\n",
    "print(f\"   Recall:    {relevant_found_c}/{total_relevant_c} = {recall_c:.3f}\")\n",
    "print(f\"   F1:        {f1_c:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
