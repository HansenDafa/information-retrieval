{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {\n",
    "    \"D1\": \"Machine learning improves search engines.\",\n",
    "    \"D2\": \"Information retrieval techniques are evolving.\",\n",
    "    \"D3\": \"Search engines use advanced algorithms.\",\n",
    "    \"D4\": \"Deep learning and neural networks are popular.\",\n",
    "    \"D5\": \"Boolean retrieval uses logical operators.\",\n",
    "    \"D6\": \"Query processing is essential in search engines.\",\n",
    "    \"D7\": \"Text mining and NLP are related to information retrieval.\",\n",
    "    \"D8\": \"Search algorithms improve information discovery.\",\n",
    "    \"D9\": \"Data science leverages machine learning.\",\n",
    "    \"D10\": \"Ranking methods optimize search engine results.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Search AND Engine\"\n",
    "query2 = \"Information OR Retrieval\"\n",
    "query3 = \"Machine NOT Learning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Ground Truth\n",
    "ground_truth = {\n",
    "    \"Search AND Engine\": [\"D3\", \"D6\", \"D10\"],  # Corrected: D1 removed\n",
    "    \"Information OR Retrieval\": [\"D2\", \"D5\", \"D7\", \"D8\"],\n",
    "    \"Machine NOT Learning\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = query_parts[0].lower()  # First term\n",
    "    operator = query_parts[1].upper()  # AND, OR, NOT\n",
    "    term2 = query_parts[2].lower()  # Second term\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        doc_text_lower = doc_text.lower()\n",
    "        term1_present = term1 in doc_text_lower\n",
    "        term2_present = term2 in doc_text_lower\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'Search AND Engine': ['D1', 'D3', 'D6', 'D10']\n",
      "Results for 'Information OR Retrieval': ['D2', 'D5', 'D7', 'D8']\n",
      "Results for 'Machine NOT Learning': []\n"
     ]
    }
   ],
   "source": [
    "results1 = simple_boolean_retrieval(query1, documents)\n",
    "results2 = simple_boolean_retrieval(query2, documents)\n",
    "results3 = simple_boolean_retrieval(query3, documents)\n",
    "\n",
    "\n",
    "print(f\"Results for '{query1}': {results1}\")\n",
    "print(f\"Results for '{query2}': {results2}\")\n",
    "print(f\"Results for '{query3}': {results3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Download required resource for tokenization (if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"Stems the words in a given text.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)  # Return as a string\n",
    "\n",
    "\n",
    "def stem_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = stem_text(query_parts[0]).lower()  # Stem and lowercase term1\n",
    "    operator = query_parts[1].upper()  # AND, OR, NOT\n",
    "    term2 = stem_text(query_parts[2]).lower()  # Stem and lowercase term2\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        stemmed_doc_text = stem_text(doc_text).lower()\n",
    "        term1_present = term1 in stemmed_doc_text\n",
    "        term2_present = term2 in stemmed_doc_text\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'Search AND Engine': ['D1', 'D3', 'D6', 'D10']\n",
      "Results for 'Information OR Retrieval': ['D2', 'D5', 'D7', 'D8']\n",
      "Results for 'Machine NOT Learning': []\n"
     ]
    }
   ],
   "source": [
    "results1 = stem_boolean_retrieval(query1, documents)\n",
    "results2 = stem_boolean_retrieval(query2, documents)\n",
    "results3 = stem_boolean_retrieval(query3, documents)\n",
    "\n",
    "\n",
    "print(f\"Results for '{query1}': {results1}\")\n",
    "print(f\"Results for '{query2}': {results2}\")\n",
    "print(f\"Results for '{query3}': {results3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')  # Download required resource for lemmatization (if you haven't already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"Lemmatizes the words in a given text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)  # Tokenize the text into words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)  # Return as a string\n",
    "\n",
    "\n",
    "def lemmatization_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = lemmatize_text(query_parts[0]).lower()  # Lemmatize and lowercase term1\n",
    "    operator = query_parts[1].upper()  # AND, OR, NOT\n",
    "    term2 = lemmatize_text(query_parts[2]).lower()  # Lemmatize and lowercase term2\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        lemmatized_doc_text = lemmatize_text(doc_text).lower()\n",
    "        term1_present = term1 in lemmatized_doc_text\n",
    "        term2_present = term2 in lemmatized_doc_text\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'Search AND Engine': ['D1', 'D3', 'D6', 'D10']\n",
      "Results for 'Information OR Retrieval': ['D2', 'D5', 'D7', 'D8']\n",
      "Results for 'Machine NOT Learning': []\n"
     ]
    }
   ],
   "source": [
    "results1 = lemmatization_boolean_retrieval(query1, documents)\n",
    "results2 = lemmatization_boolean_retrieval(query2, documents)\n",
    "results3 = lemmatization_boolean_retrieval(query3, documents)\n",
    "\n",
    "\n",
    "print(f\"Results for '{query1}': {results1}\")\n",
    "print(f\"Results for '{query2}': {results2}\")\n",
    "print(f\"Results for '{query3}': {results3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop Words Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords') # Download stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Removes stop words from a given text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Use NLTK's English stop words\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def stop_words_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = remove_stopwords(query_parts[0]).lower()  # Remove stopwords, lowercase term1\n",
    "    operator = query_parts[1].upper()  # AND, OR, NOT\n",
    "    term2 = remove_stopwords(query_parts[2]).lower()  # Remove stopwords, lowercase term2\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        processed_doc_text = remove_stopwords(doc_text).lower()\n",
    "        term1_present = term1 in processed_doc_text\n",
    "        term2_present = term2 in processed_doc_text\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for 'Search AND Engine': ['D1', 'D3', 'D6', 'D10']\n",
      "Results for 'Information OR Retrieval': ['D2', 'D5', 'D7', 'D8']\n",
      "Results for 'Machine NOT Learning': []\n"
     ]
    }
   ],
   "source": [
    "results1 = stop_words_boolean_retrieval(query1, documents)\n",
    "results2 = stop_words_boolean_retrieval(query2, documents)\n",
    "results3 = stop_words_boolean_retrieval(query3, documents)\n",
    "\n",
    "\n",
    "print(f\"Results for '{query1}': {results1}\")\n",
    "print(f\"Results for '{query2}': {results2}\")\n",
    "print(f\"Results for '{query3}': {results3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Hansen\n",
      "[nltk_data]     Dafa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Query  Simple (P)  Simple (R)  Simple (A)  Simple (F1)  \\\n",
      "0         Search AND Engine        0.75         1.0         0.9     0.857143   \n",
      "1  Information OR Retrieval        1.00         1.0         1.0     1.000000   \n",
      "2      Machine NOT Learning        0.00         0.0         1.0     0.000000   \n",
      "\n",
      "   Stemming (P)  Stemming (R)  Stemming (A)  Stemming (F1)  Lemmatization (P)  \\\n",
      "0          0.75           1.0           0.9       0.857143               0.75   \n",
      "1          1.00           1.0           1.0       1.000000               1.00   \n",
      "2          0.00           0.0           1.0       0.000000               0.00   \n",
      "\n",
      "   Lemmatization (R)  Lemmatization (A)  Lemmatization (F1)  Stop Words (P)  \\\n",
      "0                1.0                0.9            0.857143            0.75   \n",
      "1                1.0                1.0            1.000000            1.00   \n",
      "2                0.0                1.0            0.000000            0.00   \n",
      "\n",
      "   Stop Words (R)  Stop Words (A)  Stop Words (F1)  \n",
      "0             1.0             0.9         0.857143  \n",
      "1             1.0             1.0         1.000000  \n",
      "2             0.0             1.0         0.000000  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd  # Import pandas\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = {\n",
    "    \"D1\": \"Machine learning improves search engines.\",\n",
    "    \"D2\": \"Information retrieval techniques are evolving.\",\n",
    "    \"D3\": \"Search engines use advanced algorithms.\",\n",
    "    \"D4\": \"Deep learning and neural networks are popular.\",\n",
    "    \"D5\": \"Boolean retrieval uses logical operators.\",\n",
    "    \"D6\": \"Query processing is essential in search engines.\",\n",
    "    \"D7\": \"Text mining and NLP are related to information retrieval.\",\n",
    "    \"D8\": \"Search algorithms improve information discovery.\",\n",
    "    \"D9\": \"Data science leverages machine learning.\",\n",
    "    \"D10\": \"Ranking methods optimize search engine results.\"\n",
    "}\n",
    "\n",
    "# Corrected Ground Truth\n",
    "ground_truth = {\n",
    "    \"Search AND Engine\": [\"D3\", \"D6\", \"D10\"],  # Corrected: D1 removed\n",
    "    \"Information OR Retrieval\": [\"D2\", \"D5\", \"D7\", \"D8\"],\n",
    "    \"Machine NOT Learning\": []\n",
    "}\n",
    "\n",
    "all_doc_ids = list(documents.keys())\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def simple_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = query_parts[0].lower()\n",
    "    operator = query_parts[1].upper()\n",
    "    term2 = query_parts[2].lower()\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        doc_text_lower = doc_text.lower()\n",
    "        term1_present = term1 in doc_text_lower\n",
    "        term2_present = term2 in doc_text_lower\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "\n",
    "def stemming_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = stem_text(query_parts[0]).lower()\n",
    "    operator = query_parts[1].upper()\n",
    "    term2 = stem_text(query_parts[2]).lower()\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        stemmed_doc_text = stem_text(doc_text).lower()\n",
    "        term1_present = term1 in stemmed_doc_text\n",
    "        term2_present = term2 in stemmed_doc_text\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "\n",
    "def lemmatization_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = lemmatize_text(query_parts[0]).lower()\n",
    "    operator = query_parts[1].upper()\n",
    "    term2 = lemmatize_text(query_parts[2]).lower()\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        lemmatized_doc_text = lemmatize_text(doc_text).lower()\n",
    "        term1_present = term1 in lemmatized_doc_text\n",
    "        term2_present = term2 in lemmatized_doc_text\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def stop_words_boolean_retrieval(query, documents):\n",
    "    results = []\n",
    "    query_parts = query.split()\n",
    "    term1 = remove_stopwords(query_parts[0]).lower()\n",
    "    operator = query_parts[1].upper()\n",
    "    term2 = remove_stopwords(query_parts[2]).lower()\n",
    "\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        processed_doc_text = remove_stopwords(doc_text).lower()\n",
    "        term1_present = term1 in processed_doc_text\n",
    "        term2_present = term2 in processed_doc_text\n",
    "\n",
    "        if operator == \"AND\":\n",
    "            if term1_present and term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"OR\":\n",
    "            if term1_present or term2_present:\n",
    "                results.append(doc_id)\n",
    "        elif operator == \"NOT\":\n",
    "            if term1_present and not term2_present:\n",
    "                results.append(doc_id)\n",
    "        else:\n",
    "            print(\"Invalid operator\")\n",
    "            return []\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate(results, ground_truth, all_doc_ids):\n",
    "    relevant_retrieved = set(results) & set(ground_truth)\n",
    "\n",
    "    true_positives = len(relevant_retrieved)\n",
    "    false_positives = len(results) - true_positives\n",
    "    false_negatives = len(ground_truth) - true_positives\n",
    "    true_negatives = len(set(all_doc_ids) - set(results) ) - false_negatives\n",
    "\n",
    "    if len(results) > 0:\n",
    "        precision = true_positives / len(results)\n",
    "    else:\n",
    "        precision = 0.0\n",
    "\n",
    "    if len(ground_truth) > 0:\n",
    "        recall = true_positives / len(ground_truth)\n",
    "    else:\n",
    "        recall = 0.0\n",
    "\n",
    "    accuracy = (true_positives + true_negatives) / len(all_doc_ids)\n",
    "    if (precision + recall) > 0:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the queries and methods\n",
    "queries = [\"Search AND Engine\", \"Information OR Retrieval\", \"Machine NOT Learning\"]\n",
    "methods = {\n",
    "    \"Simple\": simple_boolean_retrieval,\n",
    "    \"Stemming\": stemming_boolean_retrieval,\n",
    "    \"Lemmatization\": lemmatization_boolean_retrieval,\n",
    "    \"Stop Words\": stop_words_boolean_retrieval,\n",
    "}\n",
    "\n",
    "# Store the results in a list of dictionaries\n",
    "data = []\n",
    "for query in queries:\n",
    "    row = {\"Query\": query}\n",
    "    for name, method in methods.items():\n",
    "        results = method(query, documents)\n",
    "        metrics = evaluate(results, ground_truth[query], all_doc_ids)\n",
    "        row[f\"{name} (P)\"] = metrics['precision']\n",
    "        row[f\"{name} (R)\"] = metrics['recall']\n",
    "        row[f\"{name} (A)\"] = metrics['accuracy']\n",
    "        row[f\"{name} (F1)\"] = metrics['f1_score']\n",
    "    data.append(row)\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
